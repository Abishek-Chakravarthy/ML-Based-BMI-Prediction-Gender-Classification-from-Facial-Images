{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The code is split into parts, we save into csv file output at multiple places.\n",
    "1. Part 1\n",
    "   1. Used to extract features to fn-dataset2\n",
    "   2. split across multiple files (18 files in mycase)\n",
    "   3. the extract_range function can be used to extract all at once by giving full range\n",
    "2. Load the fn-dataset2 and apply feature selection\n",
    "   1. load csv files containing pattern extracted_features_*.csv from fn-dataset2\n",
    "   2. Use pretrained DFP model to extract features and save it to  extracted_features_with_bmi.csv\n",
    "3. Load extracted_features_with_bmi.csv and run linear reg after applying PCA\n",
    "4. Classify based in BMI\n",
    "5. Some tests with our pirctures (Does not seem to work for underwight well after applying PCA)\n",
    "    1. Target Transformation (non-linear) testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & initial Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:32.253183Z",
     "iopub.status.busy": "2024-12-12T10:11:32.252795Z",
     "iopub.status.idle": "2024-12-12T10:11:37.595720Z",
     "shell.execute_reply": "2024-12-12T10:11:37.594614Z",
     "shell.execute_reply.started": "2024-12-12T10:11:32.253145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:37.598093Z",
     "iopub.status.busy": "2024-12-12T10:11:37.597590Z",
     "iopub.status.idle": "2024-12-12T10:11:38.794624Z",
     "shell.execute_reply": "2024-12-12T10:11:38.793105Z",
     "shell.execute_reply.started": "2024-12-12T10:11:37.598056Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load Pre-trained InceptionV3 model (close variant to InceptionResNetV2)\n",
    "model0 = models.inception_v3(pretrained=True)\n",
    "model0.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:38.796617Z",
     "iopub.status.busy": "2024-12-12T10:11:38.796237Z",
     "iopub.status.idle": "2024-12-12T10:11:39.339512Z",
     "shell.execute_reply": "2024-12-12T10:11:39.338373Z",
     "shell.execute_reply.started": "2024-12-12T10:11:38.796581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "csv_path = '/kaggle/input/illinois-doc-labeled-faces-dataset/person.csv'\n",
    "df = pd.read_csv(csv_path,sep = ';')\n",
    "\n",
    "myD = df[['id','sex','height','weight']]\n",
    "myD.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.343084Z",
     "iopub.status.busy": "2024-12-12T10:11:39.342609Z",
     "iopub.status.idle": "2024-12-12T10:11:39.395571Z",
     "shell.execute_reply": "2024-12-12T10:11:39.394373Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.343034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert height from inches to meters and weight from weight from pounds to kilograms\n",
    "myD['height_m'] = myD['height'] * 0.0254\n",
    "myD['weight_kg'] = myD['weight'] * 0.453592\n",
    "\n",
    " # Calculating BMI \n",
    "myD['BMI'] = myD['weight_kg'] / (myD['height_m'] ** 2)\n",
    "\n",
    "# Encode gender\n",
    "myD['sex'] = myD['sex'].fillna('')\n",
    "myD['Gender'] = myD['sex'].apply(lambda x: 1 if str(x).strip().lower() == 'male' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add path to front and side images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.397107Z",
     "iopub.status.busy": "2024-12-12T10:11:39.396754Z",
     "iopub.status.idle": "2024-12-12T10:11:39.566484Z",
     "shell.execute_reply": "2024-12-12T10:11:39.565468Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.397075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Define image directories\n",
    "front_dir = '/kaggle/input/illinois-doc-labeled-faces-dataset/front/front'\n",
    "side_dir = '/kaggle/input/illinois-doc-labeled-faces-dataset/side/side'\n",
    "\n",
    "# Add image paths to the DataFrame\n",
    "myD['front_image'] = myD['id'].apply(lambda x: os.path.join(front_dir, f'{x}.jpg'))\n",
    "myD['side_image'] = myD['id'].apply(lambda x: os.path.join(side_dir, f'{x}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.567998Z",
     "iopub.status.busy": "2024-12-12T10:11:39.567668Z",
     "iopub.status.idle": "2024-12-12T10:11:39.583337Z",
     "shell.execute_reply": "2024-12-12T10:11:39.582205Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.567966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "myD.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.585435Z",
     "iopub.status.busy": "2024-12-12T10:11:39.585045Z",
     "iopub.status.idle": "2024-12-12T10:11:39.614892Z",
     "shell.execute_reply": "2024-12-12T10:11:39.613642Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.585374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define image preprocessing pipeline for PyTorch\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),  # Resize to 299x299 for InceptionV3\n",
    "    transforms.CenterCrop(299),  # Crop to 299x299\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Assume 'model' is your pretrained InceptionV3 or any model you're using.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model0.to(device)  # Move the model to the GPU or CPU\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_image(path):\n",
    "    if os.path.exists(path) and path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            img = preprocess(img)  # Apply preprocessing\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            return img\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"Unidentified image error with file: {path}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {path}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract features using InceptionV3 (FaceNet-like)\n",
    "def extract_features(image_path):\n",
    "    img = load_image(image_path)\n",
    "    if img is not None:\n",
    "        img = img.to(device)  # Move the image to the same device as the model\n",
    "        with torch.no_grad():  # No need to track gradients during inference\n",
    "            features = model0(img)  # Extract features (raw output)\n",
    "        features = features.squeeze().cpu().numpy()  # Move back to CPU and convert to numpy\n",
    "        return features\n",
    "    return None\n",
    "\n",
    "# Function to process a range of entries with automatic file naming and feature metrics\n",
    "def extract_range(start_index, end_index, data):\n",
    "    # Initialize lists to store features and BMI values\n",
    "    front_features = []\n",
    "    side_features = []\n",
    "    bmi_values = []\n",
    "\n",
    "    # Initialize variables to store feature statistics\n",
    "    feature_lengths = []\n",
    "    feature_means = []\n",
    "    feature_stds = []\n",
    "\n",
    "    # Loop over the specified range of rows in the data DataFrame\n",
    "    for idx, row in data.iloc[start_index:end_index].iterrows():\n",
    "        front_img_features = extract_features(row['front_image'])\n",
    "        side_img_features = extract_features(row['side_image'])\n",
    "        \n",
    "        if front_img_features is not None and side_img_features is not None:\n",
    "            # Combine features from both views into a single feature vector\n",
    "            combined_features = np.concatenate((front_img_features, side_img_features))\n",
    "            front_features.append(combined_features)\n",
    "            bmi_values.append(row['BMI'])\n",
    "            \n",
    "            # Collect feature statistics\n",
    "            feature_lengths.append(len(combined_features))\n",
    "            feature_means.append(np.mean(combined_features))\n",
    "            feature_stds.append(np.std(combined_features))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(front_features)\n",
    "    y = np.array(bmi_values)\n",
    "    \n",
    "    # Create a DataFrame from the extracted features and BMI values\n",
    "    df = pd.DataFrame(X)\n",
    "    df['BMI'] = y\n",
    "    \n",
    "    # Calculate the feature statistics\n",
    "    feature_stats = {\n",
    "        'num_features': X.shape[1],  # Total number of features per row\n",
    "        'mean_features': np.mean(X),  # Mean of all features\n",
    "        'std_features': np.std(X),    # Standard deviation of all features\n",
    "        'mean_per_feature': np.mean(feature_means),  # Mean of individual feature means\n",
    "        'std_per_feature': np.mean(feature_stds),    # Mean of individual feature stds\n",
    "        'min_feature_length': np.min(feature_lengths),  # Min feature length\n",
    "        'max_feature_length': np.max(feature_lengths)   # Max feature length\n",
    "    }\n",
    "    \n",
    "    print(\"Feature Statistics:\", feature_stats)\n",
    "\n",
    "    # Generate a filename based on the range of rows processed\n",
    "    output_filename = f\"extracted_features_{start_index}_{end_index}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"Saved extracted features to {output_filename}\")\n",
    "    return feature_stats  # Return statistics for further analysis if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract in smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.616570Z",
     "iopub.status.busy": "2024-12-12T10:11:39.616187Z",
     "iopub.status.idle": "2024-12-12T10:11:39.637945Z",
     "shell.execute_reply": "2024-12-12T10:11:39.636734Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.616533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Call extract_range with range\n",
    "data = myD.copy()\n",
    "# feature_stats = extract_range(40001, 50000, data)\n",
    "\n",
    "\n",
    "###############  ADD RANDOM here\n",
    "\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FaceNet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:11:39.639625Z",
     "iopub.status.busy": "2024-12-12T10:11:39.639274Z",
     "iopub.status.idle": "2024-12-12T10:12:21.264263Z",
     "shell.execute_reply": "2024-12-12T10:12:21.262337Z",
     "shell.execute_reply.started": "2024-12-12T10:11:39.639590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to your CSV files\n",
    "path = '/kaggle/input/fn-dataset2/extracted_features_*.csv'\n",
    "\n",
    "# Use glob to match the pattern\n",
    "csv_files = glob.glob(path)\n",
    "\n",
    "print(csv_files)\n",
    "# Read each CSV file and load into a DataFrame\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# If you want to combine them into one DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CNN feature subset extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:12:21.272259Z",
     "iopub.status.busy": "2024-12-12T10:12:21.271692Z",
     "iopub.status.idle": "2024-12-12T10:12:39.456759Z",
     "shell.execute_reply": "2024-12-12T10:12:39.455494Z",
     "shell.execute_reply.started": "2024-12-12T10:12:21.272192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove rows with NaN values in the 'BMI' column\n",
    "combined_df_cleaned = combined_df.dropna(subset=['BMI'])\n",
    "\n",
    "# Now, drop the 'BMI' column to create X\n",
    "X = combined_df_cleaned.drop(columns=['BMI'])\n",
    "\n",
    "# Extract the target variable 'BMI' into y\n",
    "y = combined_df_cleaned['BMI']\n",
    "\n",
    "Y = y.copy()\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Reshape X for CNN (adding a channel dimension)\n",
    "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "print(X_reshaped.shape)\n",
    "\n",
    "if np.any(np.isnan(y)):\n",
    "    print(\"Warning: y contains NaN values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:12:39.459096Z",
     "iopub.status.busy": "2024-12-12T10:12:39.458301Z",
     "iopub.status.idle": "2024-12-12T10:13:13.352878Z",
     "shell.execute_reply": "2024-12-12T10:13:13.351716Z",
     "shell.execute_reply.started": "2024-12-12T10:12:39.459058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "feature_extractor = load_model('/kaggle/input/dfpv2.1/keras/prml_bmi_33/1/DFPv2.h5')\n",
    "\n",
    "# Extract features\n",
    "X_features = feature_extractor.predict(X_reshaped)\n",
    "\n",
    "# Flatten the features if they're multidimensional\n",
    "X_features_flat = X_features.reshape(X_features.shape[0], -1)\n",
    "\n",
    "# Create a DataFrame with the extracted features and BMI\n",
    "feature_cols = [f'feature_{i}' for i in range(X_features_flat.shape[1])]\n",
    "features_df = pd.DataFrame(X_features_flat, columns=feature_cols)\n",
    "\n",
    "# Ensure Y is converted to the correct type and shape\n",
    "Y_cleaned = np.array(Y).flatten().astype(float)\n",
    "\n",
    "# Add BMI column ensuring no type conversion issues\n",
    "features_df['BMI'] = Y_cleaned\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "features_df.to_csv('/kaggle/working/extracted_features_with_bmi.csv', index=False)\n",
    "print(\"Features shape:\", X_features_flat.shape)\n",
    "print(\"Features saved to 'extracted_features_with_bmi.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load features for linear regression; split 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:13.354873Z",
     "iopub.status.busy": "2024-12-12T10:13:13.354526Z",
     "iopub.status.idle": "2024-12-12T10:13:16.096476Z",
     "shell.execute_reply": "2024-12-12T10:13:16.095297Z",
     "shell.execute_reply.started": "2024-12-12T10:13:13.354840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('/kaggle/working/extracted_features_with_bmi.csv')\n",
    "\n",
    "# Step 2: Prepare features (X) and target (y)\n",
    "# Assuming the last column is the target variable (e.g., 'BMI')\n",
    "X = data.drop(columns=['BMI'])  # Drop target column\n",
    "y = data['BMI']  # Assuming 'BMI' is the column you want to predict\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=0.90)  \n",
    "X_pca = pca.fit_transform(X)\n",
    "# X_pca = X\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=38)\n",
    "\n",
    "# Step 5: Perform Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "\n",
    "# Step 7: Calculate Mean Squared Error (MSE) for both train and test sets\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Step 8: Calculate Mean Absolute Error (MAE) for both train and test sets\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Step 9: Calculate R-squared (R²) for both train and test sets\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Step 10: Calculate Pearson correlation coefficient for both train and test sets\n",
    "train_pearson, _ = pearsonr(y_train, y_train_pred)\n",
    "test_pearson, _ = pearsonr(y_test, y_test_pred)\n",
    "\n",
    "# Create a dictionary with the metrics for training and testing\n",
    "metrics = {\n",
    "    \"Metric\": [\"MSE\", \"MAE\", \"R²\", \"Pearson Correlation\"],\n",
    "    \"Training\": [train_mse, train_mae, train_r2, train_pearson],\n",
    "    \"Testing\": [test_mse, test_mae, test_r2, test_pearson]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(metrics_df)\n",
    "\n",
    "# Step 11: Plot results\n",
    "# Plot for training data\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Training Data: Actual vs Predicted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, color='blue', label='Training Data')\n",
    "plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Training Data: Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Testing Data: Actual vs Predicted\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_test_pred, color='green', label='Testing Data')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Testing Data: Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:16.098105Z",
     "iopub.status.busy": "2024-12-12T10:13:16.097766Z",
     "iopub.status.idle": "2024-12-12T10:13:17.264205Z",
     "shell.execute_reply": "2024-12-12T10:13:17.263062Z",
     "shell.execute_reply.started": "2024-12-12T10:13:16.098071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "def classify_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif 18.5 <= bmi < 25:\n",
    "        return \"Normal\"\n",
    "    else:\n",
    "        return \"Overweight (Obese)\"\n",
    "\n",
    "# Classify actual and predicted BMI values\n",
    "y_train_class = [classify_bmi(bmi) for bmi in y_train]\n",
    "y_train_pred_class = [classify_bmi(bmi) for bmi in y_train_pred]\n",
    "y_test_class = [classify_bmi(bmi) for bmi in y_test]\n",
    "y_test_pred_class = [classify_bmi(bmi) for bmi in y_test_pred]\n",
    "\n",
    "\n",
    "\n",
    "# Step 13: Calculate accuracy for BMI classification\n",
    "train_accuracy = accuracy_score(y_train_class, y_train_pred_class)\n",
    "test_accuracy = accuracy_score(y_test_class, y_test_pred_class)\n",
    "\n",
    "print(f\"Training BMI Classification Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing BMI Classification Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 14: Plot confusion matrices for classification accuracy\n",
    "\n",
    "# Confusion Matrix for Training Data\n",
    "train_cm = confusion_matrix(y_train_class, y_train_pred_class, labels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"])\n",
    "# Confusion Matrix for Testing Data\n",
    "test_cm = confusion_matrix(y_test_class, y_test_pred_class, labels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"])\n",
    "\n",
    "# Plot for Training Data Confusion Matrix\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"], yticklabels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"])\n",
    "plt.title(\"Training Data Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "\n",
    "# Plot for Testing Data Confusion Matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"], yticklabels=[\"Underweight\", \"Normal\", \"Overweight (Obese)\"])\n",
    "plt.title(\"Testing Data Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:17.267189Z",
     "iopub.status.busy": "2024-12-12T10:13:17.266357Z",
     "iopub.status.idle": "2024-12-12T10:13:17.283006Z",
     "shell.execute_reply": "2024-12-12T10:13:17.281776Z",
     "shell.execute_reply.started": "2024-12-12T10:13:17.267126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features2(image_path, model0, device):\n",
    "    \"\"\"\n",
    "    This function takes an image path, model0 (InceptionV3 model), and device (CPU/GPU), \n",
    "    loads and preprocesses the image, and returns the extracted features.\n",
    "    \"\"\"\n",
    "    img = load_image(image_path)\n",
    "    if img is not None:\n",
    "        img = img.to(device)  # Move the image to the same device as the model\n",
    "        with torch.no_grad():  # No need to track gradients during inference\n",
    "            features = model0(img)  # Extract features (raw output)\n",
    "        features = features.squeeze().cpu().numpy()  # Move back to CPU and convert to numpy\n",
    "        return features\n",
    "    return None\n",
    "\n",
    "def oursTst(front_img_path, side_img_path, bmi_value, regressor, feature_extractor, scaler, model0, device):\n",
    "    \"\"\"\n",
    "    This function takes in paths for front and side images, BMI value, a regressor model, \n",
    "    a feature extractor model, and a scaler, and returns predictions and actual BMI values.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the front and side images\n",
    "    front_img_features = extract_features2(front_img_path, model0, device)\n",
    "    side_img_features = extract_features2(side_img_path, model0, device)\n",
    "\n",
    "    if front_img_features is not None and side_img_features is not None:\n",
    "        # Combine features from both views into a single feature vector\n",
    "        combined_features = np.concatenate((front_img_features, side_img_features))\n",
    "\n",
    "        # Normalize the features\n",
    "        rand_X_scaled = scaler.transform([combined_features])\n",
    "        \n",
    "        \n",
    "        # Reshape X for CNN (adding a channel dimension)\n",
    "        rand_X_reshaped = rand_X_scaled.reshape(rand_X_scaled.shape[0], rand_X_scaled.shape[1], 1)\n",
    "\n",
    "            \n",
    "        # Extract features from the CNN model\n",
    "        extracted_features = feature_extractor.predict(rand_X_reshaped)\n",
    "\n",
    "        extracted_features = pca.transform(extracted_features)\n",
    "        # Make predictions\n",
    "        pred = regressor.predict(extracted_features)[0]\n",
    "        act = int(bmi_value)  # The actual BMI value\n",
    "\n",
    "        # Classify both predicted and actual BMI\n",
    "        pred_class = classify_bmi(pred)\n",
    "        act_class = classify_bmi(act)\n",
    "\n",
    "        # Plot the front and side images with captions\n",
    "        img_front = Image.open(front_img_path)\n",
    "        img_side = Image.open(side_img_path)\n",
    "\n",
    "        # Create a figure with two subplots (side by side)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns\n",
    "\n",
    "        # Plot the front image\n",
    "        axes[0].imshow(img_front)\n",
    "        axes[0].axis('off')  # Hide axes\n",
    "        axes[0].set_title('Front Image')\n",
    "\n",
    "        # Plot the side image\n",
    "        axes[1].imshow(img_side)\n",
    "        axes[1].axis('off')  # Hide axes\n",
    "        axes[1].set_title('Side Image')\n",
    "\n",
    "        # Add caption below each image\n",
    "        # axes[0].text(0.5, -0.1, f'Predicted: {pred}, Actual: {act}', ha='center', va='center', transform=axes[0].transAxes, fontsize=12)\n",
    "        # axes[1].text(0.5, -0.1, f'Predicted: {pred}, Actual: {act}', ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "\n",
    "        axes[0].text(0.5, -0.1, f'Predicted: {pred} ({pred_class}),\\n Actual: {act} ({act_class})', \n",
    "                     ha='center', va='center', transform=axes[0].transAxes, fontsize=12)\n",
    "        axes[1].text(0.5, -0.1, f'Predicted: {pred} ({pred_class}),\\n Actual: {act} ({act_class})', \n",
    "                     ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return pred, act\n",
    "    else:\n",
    "        print(\"Error: Could not extract features from one or both images.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASHIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:50:18.021183Z",
     "iopub.status.busy": "2024-12-12T11:50:18.020684Z",
     "iopub.status.idle": "2024-12-12T11:50:19.346654Z",
     "shell.execute_reply": "2024-12-12T11:50:19.345332Z",
     "shell.execute_reply.started": "2024-12-12T11:50:18.021138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred, act = oursTst(\"/kaggle/input/penguinsset/ashiq_front.jpeg\", \"/kaggle/input/penguinsset/ashiq_side.jpeg\", 18, regressor, feature_extractor, scaler, model0, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AJMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:50:10.126803Z",
     "iopub.status.busy": "2024-12-12T11:50:10.126285Z",
     "iopub.status.idle": "2024-12-12T11:50:11.406361Z",
     "shell.execute_reply": "2024-12-12T11:50:11.405121Z",
     "shell.execute_reply.started": "2024-12-12T11:50:10.126763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred, act = oursTst(\"/kaggle/input/penguinsset/ajmal_front.jpeg\", \"/kaggle/input/penguinsset/ajmal_side.jpeg\", 20, regressor, feature_extractor, scaler, model0, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rand from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:20.581037Z",
     "iopub.status.busy": "2024-12-12T10:13:20.580646Z",
     "iopub.status.idle": "2024-12-12T10:13:20.587303Z",
     "shell.execute_reply": "2024-12-12T10:13:20.586122Z",
     "shell.execute_reply.started": "2024-12-12T10:13:20.581003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def wrapper(x):\n",
    "    imgPath = myD.iloc[x].id\n",
    "    # Specify the image paths\n",
    "    imgPath_front = f'/kaggle/input/illinois-doc-labeled-faces-dataset/front/front/{imgPath}.jpg'\n",
    "    imgPath_side = f'/kaggle/input/illinois-doc-labeled-faces-dataset/side/side/{imgPath}.jpg'\n",
    "    pred, act = oursTst(imgPath_front,imgPath_side, myD.iloc[x].BMI , regressor, feature_extractor, scaler, model0, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:20.588972Z",
     "iopub.status.busy": "2024-12-12T10:13:20.588657Z",
     "iopub.status.idle": "2024-12-12T10:13:21.961950Z",
     "shell.execute_reply": "2024-12-12T10:13:21.960753Z",
     "shell.execute_reply.started": "2024-12-12T10:13:20.588941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapper(600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:21.963698Z",
     "iopub.status.busy": "2024-12-12T10:13:21.963328Z",
     "iopub.status.idle": "2024-12-12T10:13:23.087778Z",
     "shell.execute_reply": "2024-12-12T10:13:23.086533Z",
     "shell.execute_reply.started": "2024-12-12T10:13:21.963663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapper(700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:13:23.089627Z",
     "iopub.status.busy": "2024-12-12T10:13:23.089213Z",
     "iopub.status.idle": "2024-12-12T10:13:24.298034Z",
     "shell.execute_reply": "2024-12-12T10:13:24.296703Z",
     "shell.execute_reply.started": "2024-12-12T10:13:23.089585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapper(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:52:22.363500Z",
     "iopub.status.busy": "2024-12-12T11:52:22.362955Z",
     "iopub.status.idle": "2024-12-12T11:52:25.045463Z",
     "shell.execute_reply": "2024-12-12T11:52:25.044284Z",
     "shell.execute_reply.started": "2024-12-12T11:52:22.363453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/working/extracted_features_with_bmi.csv')\n",
    "\n",
    "\n",
    "# Conditional adjustment function for target\n",
    "def adjust_target(target):\n",
    "    return np.where(target < 20, target + 50/target, target + 0)\n",
    "\n",
    "def invert_adjustment(target):\n",
    "    return np.where(target < 25, target - 50/target, \n",
    "                    np.where((target >= 25) & (target <= 30), target-30/target, target))\n",
    "\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['BMI'])  # Drop target column\n",
    "y = adjust_target(data['BMI'])  # Apply conditional adjustment\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.90)  \n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=38)\n",
    "\n",
    "# Perform Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and invert adjustment\n",
    "y_train_pred = invert_adjustment(regressor.predict(X_train))\n",
    "y_test_pred = invert_adjustment(regressor.predict(X_test))\n",
    "\n",
    "# Original target values (for comparison)\n",
    "y_train_original = invert_adjustment(y_train)\n",
    "y_test_original = invert_adjustment(y_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train_original, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test_original, y_test_pred)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_original, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test_original, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train_original, y_train_pred)\n",
    "test_r2 = r2_score(y_test_original, y_test_pred)\n",
    "\n",
    "train_pearson, _ = pearsonr(y_train_original, y_train_pred)\n",
    "test_pearson, _ = pearsonr(y_test_original, y_test_pred)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics = {\n",
    "    \"Metric\": [\"MSE\", \"MAE\", \"R²\", \"Pearson Correlation\"],\n",
    "    \"Training\": [train_mse, train_mae, train_r2, train_pearson],\n",
    "    \"Testing\": [test_mse, test_mae, test_r2, test_pearson]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Print the metrics\n",
    "print(metrics_df)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Training Data: Actual vs Predicted\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train_original, y_train_pred, color='blue', label='Training Data')\n",
    "plt.plot([min(y_train_original), max(y_train_original)], \n",
    "         [min(y_train_original), max(y_train_original)], \n",
    "         color='red', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Training Data: Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Testing Data: Actual vs Predicted\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_original, y_test_pred, color='green', label='Testing Data')\n",
    "plt.plot([min(y_test_original), max(y_test_original)], \n",
    "         [min(y_test_original), max(y_test_original)], \n",
    "         color='red', lw=2, label='Ideal Fit')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Testing Data: Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:52:27.287426Z",
     "iopub.status.busy": "2024-12-12T11:52:27.286951Z",
     "iopub.status.idle": "2024-12-12T11:52:32.118354Z",
     "shell.execute_reply": "2024-12-12T11:52:32.117139Z",
     "shell.execute_reply.started": "2024-12-12T11:52:27.287369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def oursTst(front_img_path, side_img_path, bmi_value, regressor, feature_extractor, scaler, model0, device):\n",
    "\n",
    "    # Load and preprocess the front and side images\n",
    "    front_img_features = extract_features2(front_img_path, model0, device)\n",
    "    side_img_features = extract_features2(side_img_path, model0, device)\n",
    "    \n",
    "    if front_img_features is not None and side_img_features is not None:\n",
    "        # Combine features from both views into a single feature vector\n",
    "        combined_features = np.concatenate((front_img_features, side_img_features))\n",
    "        \n",
    "        # Normalize the features\n",
    "        rand_X_scaled = scaler.transform([combined_features])\n",
    "        \n",
    "        # Reshape X for CNN (adding a channel dimension)\n",
    "        rand_X_reshaped = rand_X_scaled.reshape(rand_X_scaled.shape[0], rand_X_scaled.shape[1], 1)\n",
    "        \n",
    "        # Extract features from the CNN model\n",
    "        extracted_features = feature_extractor.predict(rand_X_reshaped)\n",
    "        extracted_features = pca.transform(extracted_features)\n",
    "        \n",
    "        # Adjust the target before prediction\n",
    "        adjusted_bmi = adjust_target(bmi_value)\n",
    "        \n",
    "        # Make predictions and invert the adjustment\n",
    "        pred_adjusted = regressor.predict(extracted_features)[0]\n",
    "        pred = invert_adjustment(pred_adjusted)\n",
    "        \n",
    "        act = int(bmi_value)  # The actual BMI value\n",
    "        \n",
    "        # Classify both predicted and actual BMI\n",
    "        pred_class = classify_bmi(pred)\n",
    "        act_class = classify_bmi(act)\n",
    "        \n",
    "        # Plot the front and side images with captions\n",
    "        img_front = Image.open(front_img_path)\n",
    "        img_side = Image.open(side_img_path)\n",
    "        \n",
    "        # Create a figure with two subplots (side by side)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns\n",
    "        \n",
    "        # Plot the front image\n",
    "        axes[0].imshow(img_front)\n",
    "        axes[0].axis('off')  # Hide axes\n",
    "        axes[0].set_title('Front Image')\n",
    "        \n",
    "        # Plot the side image\n",
    "        axes[1].imshow(img_side)\n",
    "        axes[1].axis('off')  # Hide axes\n",
    "        axes[1].set_title('Side Image')\n",
    "        \n",
    "        # Add caption below each image\n",
    "        caption = f'Predicted: {pred:.2f} ({pred_class}),\\n Actual: {act} ({act_class})'\n",
    "        axes[0].text(0.5, -0.1, caption, \n",
    "                     ha='center', va='center', transform=axes[0].transAxes, fontsize=12)\n",
    "        axes[1].text(0.5, -0.1, caption, \n",
    "                     ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return pred, act\n",
    "    else:\n",
    "        print(\"Error: Could not extract features from one or both images.\")\n",
    "        return None, None\n",
    "pred, act = oursTst(\"/kaggle/input/penguinsset/ashiq_front.jpeg\", \"/kaggle/input/penguinsset/ashiq_side.jpeg\", 18, regressor, feature_extractor, scaler, model0, device)\n",
    "pred, act = oursTst(\"/kaggle/input/penguinsset/ajmal_front.jpeg\", \"/kaggle/input/penguinsset/ajmal_side.jpeg\", 20, regressor, feature_extractor, scaler, model0, device)\n",
    "\n",
    "def wrapper(x):\n",
    "    imgPath = myD.iloc[x].id\n",
    "    # Specify the image paths\n",
    "    imgPath_front = f'/kaggle/input/illinois-doc-labeled-faces-dataset/front/front/{imgPath}.jpg'\n",
    "    imgPath_side = f'/kaggle/input/illinois-doc-labeled-faces-dataset/side/side/{imgPath}.jpg'\n",
    "    pred, act = oursTst(imgPath_front,imgPath_side, myD.iloc[x].BMI , regressor, feature_extractor, scaler, model0, device)\n",
    "\n",
    "\n",
    "wrapper(150)\n",
    "wrapper(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:42:47.743473Z",
     "iopub.status.busy": "2024-12-12T11:42:47.742926Z",
     "iopub.status.idle": "2024-12-12T11:42:49.064789Z",
     "shell.execute_reply": "2024-12-12T11:42:49.063542Z",
     "shell.execute_reply.started": "2024-12-12T11:42:47.743423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapper(12000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 436801,
     "sourceId": 829570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6259240,
     "sourceId": 10141209,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6272568,
     "sourceId": 10159284,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 188572,
     "modelInstanceId": 166242,
     "sourceId": 194973,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
